{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WiDS 2018 Datathon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictive Analytics for Social Impact\n",
    "\n",
    "Competition on [Kaggle](https://www.kaggle.com/c/wids2018datathon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "from helpers import format_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (12,49,51,56,91,126,145,163,165,166,167,168,169,171,173,174,176,177,242,244,246,248,252,255,274,290,291,292,294,295,296,332,344,366,374,376,397,414,440,491,620,634,639,642,643,645,710,713,760,769,810,829,929,954,979,1001,1002,1003,1004,1005,1024,1037,1041,1043,1062,1086,1099,1100,1121,1129,1136,1152,1153,1166,1168,1182,1193,1204,1205,1207,1208,1216,1226,1228,1230,1232,1234) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (11,13,48,50,55,90,125,144,162,163,164,166,168,169,170,171,172,174,176,241,243,245,247,251,254,273,289,291,292,293,295,296,312,313,314,331,343,365,373,375,396,413,439,490,619,623,630,633,638,642,644,669,712,759,768,788,809,828,928,953,978,1000,1001,1002,1003,1004,1023,1036,1040,1042,1063,1085,1098,1099,1120,1128,1135,1151,1152,1165,1167,1181,1192,1204,1205,1206,1207,1215,1225,1227,1229,1231,1233) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (18255, 1235)\n",
      "Test Shape: (27285, 1234)\n"
     ]
    }
   ],
   "source": [
    "seed = 37\n",
    "\n",
    "train = pd.read_csv('data/train.csv', low_memory=False)\n",
    "test = pd.read_csv('data/test.csv', low_memory=False)\n",
    "\n",
    "print 'Train Shape: {}'.format(train.shape)\n",
    "print 'Test Shape: {}'.format(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = train['is_female']\n",
    "del train['is_female']\n",
    "\n",
    "del train['train_id']\n",
    "del test['test_id']\n",
    "\n",
    "# Remove rows/columns that are missing all data\n",
    "train = train.dropna(axis=0, how='all')\n",
    "train = train.dropna(axis=1, how='all')\n",
    "\n",
    "# Convert to categorical\n",
    "train_str = train.applymap(str)\n",
    "train_dum = pd.get_dummies(train_str)\n",
    "\n",
    "# Split into train and validation set\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_dum, label, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "ROC AUC Score: 0.884670439173\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "score = roc_auc_score(y_test, preds)\n",
    "\n",
    "print 'Logistic Regression'\n",
    "print 'ROC AUC Score: {}'.format(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "ROC AUC Score: 0.856606769533\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "score = roc_auc_score(y_test, preds)\n",
    "\n",
    "print 'Random Forest'\n",
    "print 'ROC AUC Score: {}'.format(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-layer Perceptron\n",
      "ROC AUC Score: 0.889215622446\n"
     ]
    }
   ],
   "source": [
    "# Multi-layer Perceptron\n",
    "clf = MLPClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "score = roc_auc_score(y_test, preds)\n",
    "\n",
    "print 'Multi-layer Perceptron'\n",
    "print 'ROC AUC Score: {}'.format(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier\n",
      "ROC AUC Score: 0.90253559105\n"
     ]
    }
   ],
   "source": [
    "# XGBoost Classifier with default parameters\n",
    "clf = XGBClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "score = roc_auc_score(y_test, preds)\n",
    "\n",
    "print 'XGBoost Classifier'\n",
    "print 'ROC AUC Score: {}'.format(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier\n",
      "ROC AUC Score: 0.911228090164\n"
     ]
    }
   ],
   "source": [
    "# Try max_depth=10 with XGBoost Classifier\n",
    "clf = XGBClassifier(max_depth=10)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "score = roc_auc_score(y_test, preds)\n",
    "\n",
    "print 'XGBoost Classifier'\n",
    "print 'ROC AUC Score: {}'.format(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier\n",
      "ROC AUC Score: 0.911228090164\n",
      "Run time: 21.7721663992 minutes\n"
     ]
    }
   ],
   "source": [
    "# Fit XGBoost Classifier, specifying eval_metric='auc'\n",
    "start = time.time()\n",
    "clf = XGBClassifier(max_depth=10, n_estimators=100, random_state=seed)\n",
    "clf.fit(X_train, y_train, eval_metric='auc')\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "score = roc_auc_score(y_test, preds)\n",
    "\n",
    "end = time.time()\n",
    "run_time = float(end - start)/60\n",
    "\n",
    "print 'XGBoost Classifier'\n",
    "print 'ROC AUC Score: {}'.format(score)\n",
    "print 'Run time: {:2} minutes'.format(run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier\n",
      "ROC AUC Score: 0.911228090164\n",
      "Run time: 25.4510277033 minutes\n"
     ]
    }
   ],
   "source": [
    "# Repeat above with n_jobs=2 to compare run times\n",
    "start = time.time()\n",
    "clf = XGBClassifier(max_depth=10, n_estimators=100, n_jobs=2, random_state=seed)\n",
    "clf.fit(X_train, y_train, eval_metric='auc')\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "score = roc_auc_score(y_test, preds)\n",
    "\n",
    "end = time.time()\n",
    "run_time = float(end - start)/60\n",
    "\n",
    "print 'XGBoost Classifier'\n",
    "print 'ROC AUC Score: {}'.format(score)\n",
    "print 'Run time: {} minutes'.format(run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier\n",
      "ROC AUC Score: 0.912035128077\n",
      "Run time: 53.4536111633 minutes\n"
     ]
    }
   ],
   "source": [
    "# Fit XGBoost Classifier, increase n_estimators to 200\n",
    "start = time.time()\n",
    "\n",
    "clf = XGBClassifier(max_depth=10, n_estimators=200, random_state=seed)\n",
    "clf.fit(X_train, y_train, eval_metric='auc')\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "score = roc_auc_score(y_test, preds)\n",
    "\n",
    "end = time.time()\n",
    "run_time = float(end - start)/60\n",
    "\n",
    "print 'XGBoost Classifier'\n",
    "print 'ROC AUC Score: {}'.format(score)\n",
    "print 'Run time: {} minutes'.format(run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier\n",
      "ROC AUC Score: 0.909064233888\n",
      "Run time: 43.1484755158 minutes\n"
     ]
    }
   ],
   "source": [
    "# Fit XGBoost Classifier, increase max_depth to 20, keep n_estimators at default (100)\n",
    "start = time.time()\n",
    "\n",
    "clf = XGBClassifier(max_depth=20, n_estimators=100, random_state=seed)\n",
    "clf.fit(X_train, y_train, eval_metric='auc')\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "score = roc_auc_score(y_test, preds)\n",
    "\n",
    "end = time.time()\n",
    "run_time = float(end - start)/60\n",
    "\n",
    "print 'XGBoost Classifier'\n",
    "print 'ROC AUC Score: {}'.format(score)\n",
    "print 'Run time: {} minutes'.format(run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier Tuned with GridSearchCV\n",
      "Best parameters: {'max_depth': 7}\n",
      "Best GridSearchCV ROC AUC score: 0.967213341827\n",
      "ROC AUC Score: 0.911375481962\n",
      "Run time: 139.383742003 minutes\n"
     ]
    }
   ],
   "source": [
    "# Tune max_depth (1-10) for XGBoost Classifier\n",
    "start = time.time()\n",
    "\n",
    "parameters = {'max_depth': range(1, 11, 2)}\n",
    "xgb = XGBClassifier()\n",
    "clf = GridSearchCV(xgb, parameters, scoring='roc_auc', refit=True)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "score = roc_auc_score(y_test, preds)\n",
    "\n",
    "end = time.time()\n",
    "run_time = float(end - start)/60\n",
    "\n",
    "print 'XGBoost Classifier Tuned with GridSearchCV'\n",
    "print 'Best parameters: {}'.format(clf.best_params_)\n",
    "print 'Best GridSearchCV ROC AUC score: {}'.format(clf.best_score_)\n",
    "print 'ROC AUC Score: {}'.format(score)\n",
    "print 'Run time: {} minutes'.format(run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format test set\n",
    "test_str = test.applymap(str)\n",
    "total = pd.concat([train_str, test_str], ignore_index=True)\n",
    "total_dummies = pd.get_dummies(total)\n",
    "train_dummies = total_dummies.head(len(train))\n",
    "test_dummies = total_dummies.tail(len(test))\n",
    "\n",
    "# # Export modified train and test sets to CSV to use with Keras\n",
    "# train_dummies.to_csv('data/x_train.csv', index=False)\n",
    "# pd.DataFrame(label).to_csv('data/y_train.csv', index=False)\n",
    "# test_dummies.to_csv('data/x_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model...\n",
      "\n",
      "Making predictions...\n",
      "\n",
      "Run time: 23.66 minutes\n",
      "\n",
      "Formatting submission...\n",
      "\n",
      "Program complete!\n"
     ]
    }
   ],
   "source": [
    "# Update submission number \n",
    "sub_number = 9\n",
    "\n",
    "# Fit best model to full train set\n",
    "start = time.time()\n",
    "print 'Fitting model...\\n'\n",
    "clf = XGBClassifier(max_depth=7, n_estimators=100, random_state=seed)\n",
    "clf.fit(train_dummies, label)\n",
    "\n",
    "# Make predictions\n",
    "print 'Making predictions...\\n'\n",
    "preds = clf.predict_proba(test_dummies)\n",
    "\n",
    "end = time.time()\n",
    "run_time = float(end - start)/60\n",
    "print 'Run time: {:0.2f} minutes'.format(run_time)\n",
    "\n",
    "# Format submission \n",
    "print '\\nFormatting submission...\\n'\n",
    "df = format_submission(preds, len(test_dummies))\n",
    "df.to_csv('submissions/submission{}.csv'.format(sub_number), index=False)\n",
    "\n",
    "print 'Program complete!'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
