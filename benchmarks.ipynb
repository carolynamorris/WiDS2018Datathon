{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WiDS 2018 Datathon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictive Analytics for Social Impact\n",
    "\n",
    "Competition on [Kaggle](https://www.kaggle.com/c/wids2018datathon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "from helpers import format_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (18255, 1235)\n",
      "Test Shape: (27285, 1234)\n"
     ]
    }
   ],
   "source": [
    "seed = 37\n",
    "\n",
    "train = pd.read_csv('data/train.csv', low_memory=False)\n",
    "test = pd.read_csv('data/test.csv', low_memory=False)\n",
    "\n",
    "print 'Train Shape: {}'.format(train.shape)\n",
    "print 'Test Shape: {}'.format(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Feature', 'Percentage Missing')\n",
      "('DG4_OTHERS', '1.00')\n",
      "('G2P2_2_OTHERS', '1.00')\n",
      "('G2P2_10_OTHERS', '1.00')\n",
      "('G2P2_12_OTHERS', '1.00')\n",
      "('G2P2_15_OTHERS', '1.00')\n",
      "('MT12_99', '1.00')\n",
      "('MT13_4_OTHERS', '1.00')\n",
      "('MT13_96_OTHERS', '1.00')\n",
      "('MT14_3_OTHERS', '1.00')\n",
      "('MT14_5_OTHERS', '1.00')\n",
      "('MT14_7_OTHERS', '1.00')\n",
      "('MM3_15', '1.00')\n",
      "('MM3_16', '1.00')\n",
      "('MM4_16', '1.00')\n",
      "('MM5_4', '1.00')\n",
      "('MM5_5', '1.00')\n",
      "('MM5_15', '1.00')\n",
      "('MM5_16', '1.00')\n",
      "('MM5A_4', '1.00')\n",
      "('MM5A_5', '1.00')\n",
      "('MM5A_15', '1.00')\n",
      "('MM5A_16', '1.00')\n",
      "('MM6_16', '1.00')\n",
      "('MM7_4', '1.00')\n",
      "('MM7_5', '1.00')\n",
      "('MM7_15', '1.00')\n",
      "('MM7_16', '1.00')\n",
      "('MM8_15', '1.00')\n",
      "('MM8_16', '1.00')\n",
      "('MM11_4', '1.00')\n",
      "('MM11_5', '1.00')\n",
      "('MM11_5_OTHERS', '1.00')\n",
      "('MM11_11_OTHERS', '1.00')\n",
      "('MM11_15', '1.00')\n",
      "('MM11_16', '1.00')\n",
      "('MM15_OTHERS', '1.00')\n",
      "('MM17_13', '1.00')\n",
      "('MM17_15', '1.00')\n",
      "('MM17_17', '1.00')\n",
      "('MM17_19', '1.00')\n",
      "('MM17_22', '1.00')\n",
      "('MM17_96', '1.00')\n",
      "('MM17A', '1.00')\n",
      "('MM38_OTHERS', '1.00')\n",
      "('MM40_14', '1.00')\n",
      "('MM40_96', '1.00')\n",
      "('MMP4_7', '1.00')\n",
      "('MMP4_8', '1.00')\n",
      "('MMP4_96', '1.00')\n",
      "('FB28_3_OTHERS', '1.00')\n",
      "Number of features missing 100% of data: 50\n",
      "Total number of features: 1235\n"
     ]
    }
   ],
   "source": [
    "# How much data is each train set feature missing?\n",
    "size = len(train)\n",
    "insufficient_cols = []\n",
    "print('Feature', 'Percentage Missing')\n",
    "for col in train.columns:\n",
    "    num_present = len(train[col].dropna(how='any', axis=0))\n",
    "    proportion = 1-(float(num_present)/size)\n",
    "    if proportion >= 1:\n",
    "        insufficient_cols.append(col)\n",
    "        print(col, '{:.2f}'.format(proportion))\n",
    "print 'Number of features missing 100% of data: {}'.format(len(insufficient_cols))\n",
    "print 'Total number of features: {}'.format(train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shape train set: (18255, 1185)\n",
      "New shape test set: (18255, 1185)\n"
     ]
    }
   ],
   "source": [
    "# Drop the columns missing 100% of data\n",
    "train.drop(insufficient_cols, axis=1, inplace=True)\n",
    "test.drop(insufficient_cols, axis=1, inplace=True)\n",
    "print 'New shape train set: {}'.format(train.shape)\n",
    "print 'New shape test set: {}'.format(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = train['is_female']\n",
    "del train['is_female']\n",
    "\n",
    "del train['train_id']\n",
    "del test['test_id']\n",
    "\n",
    "# Remove rows/columns that are missing all data\n",
    "train = train.dropna(axis=0, how='all')\n",
    "train = train.dropna(axis=1, how='all')\n",
    "\n",
    "# Convert to categorical\n",
    "train_str = train.applymap(str)\n",
    "train_dum = pd.get_dummies(train_str)\n",
    "\n",
    "# Split into train and validation set\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_dum, label, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-NN\n",
      "ROC AUC Score: 0.774295232101\n"
     ]
    }
   ],
   "source": [
    "# k-NN\n",
    "clf = KNeighborsClassifier(n_neighbors=10)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "score = roc_auc_score(y_test, preds)\n",
    "\n",
    "print 'k-NN'\n",
    "print 'ROC AUC Score: {}'.format(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "ROC AUC Score: 0.884670439173\n",
      "Run time: 0.429002416134 minutes\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "start = time.time()\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "score = roc_auc_score(y_test, preds)\n",
    "\n",
    "end = time.time()\n",
    "run_time = float(end - start)/60\n",
    "\n",
    "print 'Logistic Regression'\n",
    "print 'ROC AUC Score: {}'.format(score)\n",
    "print 'Run time: {} minutes'.format(run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "ROC AUC Score: 0.857780478454\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "score = roc_auc_score(y_test, preds)\n",
    "\n",
    "print 'Random Forest'\n",
    "print 'ROC AUC Score: {}'.format(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-layer Perceptron\n",
      "ROC AUC Score: 0.882140364013\n"
     ]
    }
   ],
   "source": [
    "# Multi-layer Perceptron\n",
    "clf = MLPClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "score = roc_auc_score(y_test, preds)\n",
    "\n",
    "print 'Multi-layer Perceptron'\n",
    "print 'ROC AUC Score: {}'.format(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier\n",
      "ROC AUC Score: 0.903562812416\n"
     ]
    }
   ],
   "source": [
    "# XGBoost Classifier with default parameters\n",
    "clf = XGBClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "score = roc_auc_score(y_test, preds)\n",
    "\n",
    "print 'XGBoost Classifier'\n",
    "print 'ROC AUC Score: {}'.format(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier\n",
      "ROC AUC Score: 0.911228090164\n"
     ]
    }
   ],
   "source": [
    "# Try max_depth=10 with XGBoost Classifier\n",
    "clf = XGBClassifier(max_depth=10)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "score = roc_auc_score(y_test, preds)\n",
    "\n",
    "print 'XGBoost Classifier'\n",
    "print 'ROC AUC Score: {}'.format(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier\n",
      "ROC AUC Score: 0.911228090164\n",
      "Run time: 21.7721663992 minutes\n"
     ]
    }
   ],
   "source": [
    "# Fit XGBoost Classifier, specifying eval_metric='auc'\n",
    "start = time.time()\n",
    "clf = XGBClassifier(max_depth=10, n_estimators=100, random_state=seed)\n",
    "clf.fit(X_train, y_train, eval_metric='auc')\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "score = roc_auc_score(y_test, preds)\n",
    "\n",
    "end = time.time()\n",
    "run_time = float(end - start)/60\n",
    "\n",
    "print 'XGBoost Classifier'\n",
    "print 'ROC AUC Score: {}'.format(score)\n",
    "print 'Run time: {:2} minutes'.format(run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier\n",
      "ROC AUC Score: 0.911228090164\n",
      "Run time: 25.4510277033 minutes\n"
     ]
    }
   ],
   "source": [
    "# Repeat above with n_jobs=2 to compare run times\n",
    "start = time.time()\n",
    "clf = XGBClassifier(max_depth=10, n_estimators=100, n_jobs=2, random_state=seed)\n",
    "clf.fit(X_train, y_train, eval_metric='auc')\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "score = roc_auc_score(y_test, preds)\n",
    "\n",
    "end = time.time()\n",
    "run_time = float(end - start)/60\n",
    "\n",
    "print 'XGBoost Classifier'\n",
    "print 'ROC AUC Score: {}'.format(score)\n",
    "print 'Run time: {} minutes'.format(run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier\n",
      "ROC AUC Score: 0.912035128077\n",
      "Run time: 53.4536111633 minutes\n"
     ]
    }
   ],
   "source": [
    "# Fit XGBoost Classifier, increase n_estimators to 200\n",
    "start = time.time()\n",
    "\n",
    "clf = XGBClassifier(max_depth=10, n_estimators=200, random_state=seed)\n",
    "clf.fit(X_train, y_train, eval_metric='auc')\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "score = roc_auc_score(y_test, preds)\n",
    "\n",
    "end = time.time()\n",
    "run_time = float(end - start)/60\n",
    "\n",
    "print 'XGBoost Classifier'\n",
    "print 'ROC AUC Score: {}'.format(score)\n",
    "print 'Run time: {} minutes'.format(run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier\n",
      "ROC AUC Score: 0.909064233888\n",
      "Run time: 43.1484755158 minutes\n"
     ]
    }
   ],
   "source": [
    "# Fit XGBoost Classifier, increase max_depth to 20, keep n_estimators at default (100)\n",
    "start = time.time()\n",
    "\n",
    "clf = XGBClassifier(max_depth=20, n_estimators=100, random_state=seed)\n",
    "clf.fit(X_train, y_train, eval_metric='auc')\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "score = roc_auc_score(y_test, preds)\n",
    "\n",
    "end = time.time()\n",
    "run_time = float(end - start)/60\n",
    "\n",
    "print 'XGBoost Classifier'\n",
    "print 'ROC AUC Score: {}'.format(score)\n",
    "print 'Run time: {} minutes'.format(run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier Tuned with GridSearchCV\n",
      "Best parameters: {'max_depth': 7}\n",
      "Best GridSearchCV ROC AUC score: 0.967213341827\n",
      "ROC AUC Score: 0.911375481962\n",
      "Run time: 139.383742003 minutes\n"
     ]
    }
   ],
   "source": [
    "# Tune max_depth (1-10) for XGBoost Classifier\n",
    "start = time.time()\n",
    "\n",
    "parameters = {'max_depth': range(1, 11, 2)}\n",
    "xgb = XGBClassifier()\n",
    "clf = GridSearchCV(xgb, parameters, scoring='roc_auc', refit=True)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "score = roc_auc_score(y_test, preds)\n",
    "\n",
    "end = time.time()\n",
    "run_time = float(end - start)/60\n",
    "\n",
    "print 'XGBoost Classifier Tuned with GridSearchCV'\n",
    "print 'Best parameters: {}'.format(clf.best_params_)\n",
    "print 'Best GridSearchCV ROC AUC score: {}'.format(clf.best_score_)\n",
    "print 'ROC AUC Score: {}'.format(score)\n",
    "print 'Run time: {} minutes'.format(run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format test set\n",
    "test_str = test.applymap(str)\n",
    "total = pd.concat([train_str, test_str], ignore_index=True)\n",
    "total_dummies = pd.get_dummies(total)\n",
    "train_dummies = total_dummies.head(len(train))\n",
    "test_dummies = total_dummies.tail(len(test))\n",
    "\n",
    "# # Export modified train and test sets to CSV to use with Keras\n",
    "# train_dummies.to_csv('data/x_train.csv', index=False)\n",
    "# pd.DataFrame(label).to_csv('data/y_train.csv', index=False)\n",
    "# test_dummies.to_csv('data/x_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model...\n",
      "\n",
      "Making predictions...\n",
      "\n",
      "Run time: 23.66 minutes\n",
      "\n",
      "Formatting submission...\n",
      "\n",
      "Program complete!\n"
     ]
    }
   ],
   "source": [
    "# Update submission number \n",
    "sub_number = 9\n",
    "\n",
    "# Fit best model to full train set\n",
    "start = time.time()\n",
    "print 'Fitting model...\\n'\n",
    "clf = XGBClassifier(max_depth=7, n_estimators=100, random_state=seed)\n",
    "clf.fit(train_dummies, label)\n",
    "\n",
    "# Make predictions\n",
    "print 'Making predictions...\\n'\n",
    "preds = clf.predict_proba(test_dummies)\n",
    "\n",
    "end = time.time()\n",
    "run_time = float(end - start)/60\n",
    "print 'Run time: {:0.2f} minutes'.format(run_time)\n",
    "\n",
    "# Format submission \n",
    "print '\\nFormatting submission...\\n'\n",
    "df = format_submission(preds, len(test_dummies))\n",
    "df.to_csv('submissions/submission{}.csv'.format(sub_number), index=False)\n",
    "\n",
    "print 'Program complete!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model...\n",
      "\n",
      "Making predictions...\n",
      "\n",
      "\n",
      "Formatting submission...\n",
      "\n",
      "Program complete!\n",
      "\n",
      "XGBoost Classifier Tuned with GridSearchCV\n",
      "Best parameters: {'n_estimators': 250, 'max_depth': 9}\n",
      "Best GridSearchCV ROC AUC score: 0.969377013799\n",
      "Run time: 687.11 minutes\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation on full training set\n",
    "\n",
    "# Update submission number \n",
    "start = time.time()\n",
    "sub_number = 10\n",
    "\n",
    "# GridSearchCV\n",
    "print 'Tuning model...\\n'\n",
    "parameters = {'max_depth': range(1, 11, 2), 'n_estimators': range(50,300,50)}\n",
    "xgb = XGBClassifier()\n",
    "clf = GridSearchCV(xgb, parameters, scoring='roc_auc', refit=True)\n",
    "clf.fit(train_dummies, label)\n",
    "\n",
    "# Make predictions\n",
    "print 'Making predictions...\\n'\n",
    "preds = clf.predict_proba(test_dummies)\n",
    "\n",
    "# Format submission \n",
    "print 'Formatting submission...\\n'\n",
    "df = format_submission(preds, len(test_dummies))\n",
    "df.to_csv('submissions/submission{}.csv'.format(sub_number), index=False)\n",
    "\n",
    "end = time.time()\n",
    "run_time = float(end - start)/60\n",
    "\n",
    "print 'Program complete!\\n'\n",
    "print 'XGBoost Classifier Tuned with GridSearchCV'\n",
    "print 'Best parameters: {}'.format(clf.best_params_)\n",
    "print 'Best GridSearchCV ROC AUC score: {}'.format(clf.best_score_)\n",
    "print 'Run time: {:0.2f} minutes'.format(run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
